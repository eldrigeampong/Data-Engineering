{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANTE\\miniconda3\\envs\\SNOWFLAKE\\lib\\site-packages\\snowflake\\connector\\options.py:108: UserWarning: You have an incompatible version of 'pyarrow' installed (12.0.1), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "Failed to import ArrowResult. No Apache Arrow result set format can be used. ImportError: DLL load failed while importing arrow_iterator: The specified procedure could not be found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from credentials import *\n",
    "import snowflake.connector\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conn(**db_parameters):\n",
    "    \"\"\"\n",
    "    Composes a SQLAlchemy connect string from the given database connection parameters.\n",
    "\n",
    "    https://github.com/snowflakedb/snowflake-sqlalchemy#escaping-special-characters-such-as---signs-in-passwords\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(URL(**db_parameters))\n",
    "        return engine.connect()\n",
    "    \n",
    "    except snowflake.connector.errors.ProgrammingError as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "    finally:\n",
    "        engine.connect().close()\n",
    "        engine.dispose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establsih Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = make_conn(role=ROLE, account=ACCOUNT_ID, user=USERNAME, password=PASSWORD, database='', schema='', warehouse=WAREHOUSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Database and Schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1569915ffd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_name = \"aws_metro_bike_share\"\n",
    "\n",
    "create_database_query = f\"\"\"\n",
    "                         CREATE DATABASE IF NOT EXISTS {db_name}\n",
    "                         \"\"\"\n",
    "\n",
    "conn.execute(text(create_database_query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1569a7d01f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = make_conn(role=ROLE, account=ACCOUNT_ID, user=USERNAME, password=PASSWORD, database=db_name, schema='', warehouse=WAREHOUSE)\n",
    "\n",
    "schema_name = \"aws_metro_bike_share_schema\"\n",
    "\n",
    "create_schema_query = f\"\"\"\n",
    "                       CREATE SCHEMA IF NOT EXISTS {schema_name}\n",
    "                       \"\"\"\n",
    "\n",
    "conn.execute(text(create_schema_query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AWS Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AWS_S3_INTEGRATION', 'EXTERNAL_STAGE', 'STORAGE', 'true', 'AWS Storage Integration', datetime.datetime(2023, 7, 24, 13, 41, 9, 586000, tzinfo=<DstTzInfo 'America/Los_Angeles' PDT-1 day, 17:00:00 DST>))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = make_conn(role=ROLE, account=ACCOUNT_ID, user=USERNAME, password=PASSWORD, database=db_name, schema=schema_name, warehouse=WAREHOUSE)\n",
    "integration_name = 'aws_s3_integration'\n",
    "integration_comment = 'AWS Storage Integration'\n",
    "\n",
    "aws_integration_query = f\"\"\"\n",
    "                         CREATE OR REPLACE STORAGE INTEGRATION {integration_name}\n",
    "                         TYPE = EXTERNAL_STAGE\n",
    "                         STORAGE_PROVIDER = 'S3'\n",
    "                         ENABLED = TRUE\n",
    "                         STORAGE_AWS_ROLE_ARN = '{AWS_ARN}'\n",
    "                         STORAGE_ALLOWED_LOCATIONS = ('{AWS_LOCATION}')\n",
    "                         COMMENT = '{integration_comment}'\n",
    "\n",
    "                        \"\"\"\n",
    "\n",
    "conn.execute(text(aws_integration_query))\n",
    "conn.execute(text(\"\"\"SHOW INTEGRATIONS like '%s3%'\"\"\")).fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNOWFLAKE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
